{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import dependencies\n",
    "from bs4 import BeautifulSoup as bs\n",
    "from splinter import Browser\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "import time \n",
    "import requests\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[WDM] - ====== WebDriver manager ======\n",
      "[WDM] - Current google-chrome version is 90.0.4430\n",
      "[WDM] - Get LATEST driver version for 90.0.4430\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[WDM] - Driver [C:\\Users\\renuk\\.wdm\\drivers\\chromedriver\\win32\\90.0.4430.24\\chromedriver.exe] found in cache\n"
     ]
    }
   ],
   "source": [
    "# Establish a path with Chrome Driver and create browser\n",
    "executable_path={'executable_path': ChromeDriverManager().install()}\n",
    "browser=Browser('chrome', **executable_path, headless=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign URL and visit with browser\n",
    "url= \"https://mars.nasa.gov/news/?page=0&per_page=40&order=publish_date+desc%2Ccreated_at+desc&search=&category=19%2C165%2C184%2C204&blank_scope=Latest\"\n",
    "browser.visit(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an HTML object\n",
    "html = browser.html\n",
    "# Parse HTML with Beautiful Soup\n",
    "soup = bs(html, 'html.parser')\n",
    "# Scrape the [NASA Mars News Site](https://mars.nasa.gov/news/) and collect the latest News Title and Paragraph Text. Assign the text to variables.\n",
    "news_title = soup.find_all(\"div\", class_=\"content_title\")[1].text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find and print first paragraph\n",
    "news_p = soup.find(\"div\", class_=\"article_teaser_body\").text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load URL for image and visit using splinter \n",
    "image_url= \"https://www.jpl.nasa.gov/images?search=&category=Mars\"\n",
    "browser.visit(image_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the latest featured image using splinter and click on it\n",
    "time.sleep(1)\n",
    "browser.find_by_css('.mb-3').click()\n",
    "html = browser.html\n",
    "\n",
    "# Parse HTML with Beautiful Soup\n",
    "soup = bs(html, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Locate image using splinter, assign to a variable, and save and print complete URL\n",
    "featured_img= soup.find(\"img\", class_=\"BaseImage\")\n",
    "featured_img_url= featured_img['data-src']\n",
    "time.sleep(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Mars Facts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visit the Mars Facts webpage (https://space-facts.com/mars/) and assign to variable name\n",
    "facts_url= \"https://space-facts.com/mars/\"\n",
    "browser.visit(facts_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read table on webpage using Pandas and assign to variable name\n",
    "mars_facts=pd.read_html(facts_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataframe using scraped data\n",
    "facts_df = mars_facts[0]\n",
    "\n",
    "# Rename columns and set 'Category' column as index\n",
    "facts_df.columns= (['Category', 'Mars Facts'])\n",
    "mars_facts_clean=facts_df.drop(0)\n",
    "clean_df=mars_facts_clean.set_index('Category')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use Pandas to convert the data to a HTML table string and print as a file.\n",
    "html_table = facts_df.to_html('mars.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use Pandas to convert the data to a HTML table string to use in Mongo dictionary.\n",
    "html_table_mongo = facts_df.to_html()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Mars Hemispheres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Visit the USGS Astrogeology site\n",
    "astrogeo_url= \"https://astrogeology.usgs.gov/search/results?q=hemisphere+enhanced&k1=target&v1=Mars\"\n",
    "browser.visit(astrogeo_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an HTML object\n",
    "html = browser.html\n",
    "\n",
    "# Parse HTML with Beautiful Soup\n",
    "soup = bs(html, 'html.parser')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scrape the USGS Astrogeology site and collect high resolution images for each of Mar's hemispheres. \n",
    "# Save both the image url string for the full resolution hemisphere image and the Hemisphere title containing the hemisphere name. \n",
    "\n",
    "# Starting with first hemisphere listed, click on hemisphere name\n",
    "cerb_url=browser.links.find_by_partial_text('Cerberus')\n",
    "cerb_url.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sleep browser for one second and create a new HTML object on page\n",
    "time.sleep(1)\n",
    "html = browser.html\n",
    "# Parse Cerberus page HTML with Beautiful Soup\n",
    "soup_cerb = bs(html, 'html.parser')\n",
    "\n",
    "# Using BeautifulSoup, locate and save hemisphere image and title \n",
    "cerb_photo = soup_cerb.find(\"a\", text=\"Sample\").get(\"href\")\n",
    "cerb_title = soup_cerb.find(\"h2\", class_=\"title\").text\n",
    "browser.back()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "html = browser.html\n",
    "# Parse HTML with Beautiful Soup\n",
    "soup = bs(html, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "schia_url=browser.links.find_by_partial_text('Schiaparelli')\n",
    "schia_url.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "time.sleep(1)\n",
    "html = browser.html\n",
    "# Parse HTML with Beautiful Soup\n",
    "soup_schia = bs(html, 'html.parser')\n",
    "schia_photo = soup_schia.find(\"a\", text=\"Sample\").get(\"href\")\n",
    "schia_title = soup_schia.find(\"h2\", class_=\"title\").text\n",
    "#print(schia_photo)\n",
    "browser.back()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "html = browser.html\n",
    "# Parse HTML with Beautiful Soup\n",
    "soup = bs(html, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "syrtis_url=browser.links.find_by_partial_text('Syrtis')\n",
    "syrtis_url.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "time.sleep(1)\n",
    "html = browser.html\n",
    "# Parse HTML with Beautiful Soup\n",
    "soup_syrtis = bs(html, 'html.parser')\n",
    "syrtis_photo = soup_syrtis.find(\"a\", text=\"Sample\").get(\"href\")\n",
    "syrtis_title = soup_syrtis.find(\"h2\", class_=\"title\").text\n",
    "#print(syrtis_photo)\n",
    "browser.back()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "html = browser.html\n",
    "# Parse HTML with Beautiful Soup\n",
    "soup = bs(html, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "valles_url=browser.links.find_by_partial_text('Valles')\n",
    "valles_url.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "time.sleep(1)\n",
    "html = browser.html\n",
    "# Parse HTML with Beautiful Soup\n",
    "soup_valles = bs(html, 'html.parser')\n",
    "valles_photo = soup_valles.find(\"a\", text=\"Sample\").get(\"href\")\n",
    "valles_title = soup_valles.find(\"h2\", class_=\"title\").text\n",
    "#print(valles_photo)\n",
    "browser.back()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "browser.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymongo\n",
    "\n",
    "# # Connect to Mongo\n",
    "# # This needs to be done in python \n",
    "mongo_conn='mongodb://localhost:27017'\n",
    "client=pymongo.MongoClient(mongo_conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "hemisphere_image_urls=[\n",
    "    {\"title\": cerb_title, \"img_url\": cerb_photo},\n",
    "    {\"title\": schia_title, \"img_url\": schia_photo},\n",
    "    {\"title\": syrtis_title, \"img_url\": syrtis_photo},\n",
    "    {\"title\": valles_title, \"img_url\": valles_photo}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pymongo.results.InsertOneResult at 0x2074a065e80>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Store table and headlines in dictionary\n",
    "mars_facts= {\n",
    "    'Headline': news_title,\n",
    "    'News': news_p,\n",
    "    'Featured_Image': featured_img_url,\n",
    "    'Table': html_table_mongo,\n",
    "    'Hemisphere_Images': hemisphere_image_urls\n",
    "    }\n",
    "# print(mars_facts)\n",
    "# Import to MongoDB\n",
    "client.mars_db.mars.insert_one(mars_facts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# conn = 'mongodb://localhost:27017'\n",
    "# import pymongo\n",
    "# PyMongo constructor - this is a class, as noted by the capital letters\n",
    "# client = pymongo.MongoClient(conn)\n",
    "# db=client.MarsDB\n",
    "# response= requests.get(url)\n",
    "# facts_df.drop(\"index\")\n",
    "# facts_df.set_index=(['Category'])\n",
    "#Store hemisphere labels and images in a dictionary. Use a Python dictionary to store the data using the keys `img_url` and `title`.\n",
    "# hemisphere_image_urls=[\n",
    "#     {\"title\": cerb_title, \"img_url\": cerb_photo},\n",
    "#     {\"title\": schia_title, \"img_url\": schia_photo},\n",
    "#     {\"title\": syrtis_title, \"img_url\": syrtis_photo},\n",
    "#     {\"title\": valles_title, \"img_url\": valles_photo}\n",
    "  \n",
    "# ]\n",
    "# #print(hemisphere_image_urls)\n",
    "\n",
    "# Import to MongoDB\n",
    "# client.mars_db.mars.insert_one(hemisphere_image_urls)\n",
    "# Put all 4 in a loop, will click into first one, scrape the website, grab text and full size image, store those\n",
    "#i n a dictionary, then goes to next link do same thing, etc. 4 "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
